{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "insured-allen",
   "metadata": {},
   "source": [
    "<div id=top></div>\n",
    "\n",
    "# Reinforcement Learning with Doom - Increasing complexity and monitoring the model\n",
    "\n",
    "Leandro Kieliger\n",
    "contact@lkieliger.ch\n",
    "\n",
    "---\n",
    "## Description\n",
    "\n",
    "In this notebook we are going to significantly improve the learning efficiency of the setup created in the previous part of this series. First, we will see how to modify rewards to incentivize behaviors helping reach the initial goal, an method called \"reward shaping\". In the second part, we will design an adaptive learning process that varies the difficulty of the training environment based on the performance of the agent. \n",
    "\n",
    "\n",
    "### [Part 1 - Reward Shaping](#part_1)\n",
    "* [aaa](#aaa)\n",
    "* [bbb](#bbb)\n",
    "\n",
    "    \n",
    "### [Part 2 - Curriculum Learning](#part_2)\n",
    "* [ccc](#ccc)\n",
    "* [ddd](#ddd)\n",
    "    \n",
    "    \n",
    "### [Part 3 - Bonus visualisation](#part_3)\n",
    "* [eee](#eee)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-parameter",
   "metadata": {},
   "source": [
    "<div id=part_1></div>\n",
    "\n",
    "# [^](#top) Part 1 - Reward Shaping\n",
    "\n",
    "\n",
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "comparative-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cv2\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import typing as t\n",
    "import vizdoom\n",
    "from stable_baselines3 import ppo\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common import evaluation, policies\n",
    "from torch import nn\n",
    "\n",
    "from common import envs, plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-belle",
   "metadata": {},
   "source": [
    "In the previous notebook we saw that the learning process was very slow. Indeed, even after training more than 2 million steps, our agent barely reached 2 frags per match on average. In comparison, the best bot manages to get around 13 frags. \n",
    "\n",
    "We also discussed one of the main reason why the model had so much difficulties getting started. The issue is related to rewards being sparse. That is, the agent has to execute many steps \"just right\" before it can observe some meaningful reward signal. It must manage to move and aim at ennemies while repeatedly shooting them in order to (possibly) get some rewards. Such sequence of action rarely happens by chance. If rewards are rare, this means that it will take a long time to reinforce good behaviors.\n",
    "\n",
    "To solve the issue of sparse rewards, we can give our agent small positive rewards for every action we believe will be beneficial to the learning process. Here is the list of actions we would like to incentivize as well as the associated reward:\n",
    "\n",
    "| Action                     | Reward       |\n",
    "| -------------------------- |--------------| \n",
    "| Frag                       |  1 per frag   | \n",
    "| Damaging enemies           |  0.01 per damage point | \n",
    "| Picking up ammunition      |  0.02 per unit |\n",
    "| Using ammunition           | -0.01 per unit | \n",
    "| Picking up health          |  0.02 per health point |\n",
    "| Losing health              | -0.01 per health point |\n",
    "| Picking up armor           |  0.01 per armor point|\n",
    "| Moved distance > 3 units   |  5e-5 per step|\n",
    "| Moved distance < 3 units   | -2.5e-3 per step|\n",
    "\n",
    "Note that players typically have 100 health points and that damage points correspond to the number of ennemy health points that were removed. Also, players can typically move at around 16 units per tick. The distance reward is here to avoid \"camping\" behavior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
